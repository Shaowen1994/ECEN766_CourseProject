{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Clear ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Path = '../Datasets/'\n",
    "\n",
    "with open(Data_Path + 'protein_list','r') as file_1:\n",
    "    pro_lines = file_1.readlines()\n",
    "with open(Data_Path + 'seq_list','r') as file_2:\n",
    "    seq_lines = file_2.readlines()\n",
    "with open(Data_Path + 'ss_list','r') as file_3:\n",
    "    ss_lines = file_3.readlines()\n",
    "    \n",
    "seq_num = len(seq_lines)\n",
    "\n",
    "seq_clear_num = 0\n",
    "resi_clear_num = 0\n",
    "SS_num_dict = {'H':0,'E':0,'C':0}\n",
    "\n",
    "with open(Data_Path + 'protein_list_clear','w') as file_1, open(Data_Path + 'seq_list_clear','w') as file_2, open(Data_Path + 'ss_list_clear','w') as file_3:\n",
    "    for i in range(seq_num):\n",
    "        ss = ss_lines[i].strip('\\n')\n",
    "        if not 'M' in ss:\n",
    "            seq_clear_num += 1\n",
    "            resi_clear_num += len(seq_lines[i].strip('\\n'))\n",
    "            for ch in SS_num_dict.keys():\n",
    "                SS_num_dict[ch] += ss.count(ch)\n",
    "            \n",
    "            file_1.write(pro_lines[i])\n",
    "            file_2.write(seq_lines[i])\n",
    "            file_3.write(ss_lines[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence amount after data clearance: 8845\n",
      "Residue amount after data clearance: 1384022\n",
      "\n",
      "Residue amount for different secondary structure:\n",
      "H: 507813\n",
      "E: 313871\n",
      "C: 562338\n"
     ]
    }
   ],
   "source": [
    "print('Sequence amount after data clearance: %d'%seq_clear_num)\n",
    "print('Residue amount after data clearance: %d'%resi_clear_num)\n",
    "print()\n",
    "print('Residue amount for different secondary structure:')\n",
    "for ch in SS_num_dict.keys():\n",
    "    print('%s: %d'%(ch,SS_num_dict[ch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_dict = {'A':0,'R':1,'N':2,'D':3,'C':4,'Q':5,'E':6,'G':7,'H':8,'I':9,'L':10,\n",
    "           'K':11,'M':12,'F':13,'P':14,'S':15,'T':16,'W':17,'Y':18,'v':19,'!':20}\n",
    "SS_dict = {'H':0,'E':1,'C':2}\n",
    "\n",
    "def one_hot_encoding(seq,ref_dict,width=None):\n",
    "    if width == None:\n",
    "        width = len(ref_dict.keys())\n",
    "    L = len(seq)\n",
    "    result = np.zeros([L,width])\n",
    "    for i in range(L):\n",
    "        result[i][ref_dict[seq[i]]] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for Seq-SS models: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Data_Path + 'protein_list_clear','r') as file_pro:\n",
    "    pro_clear_lines = file_pro.readlines()\n",
    "with open(Data_Path + 'seq_list_clear','r') as file_seq:\n",
    "    seq_clear_lines = file_seq.readlines()\n",
    "with open(Data_Path + 'ss_list_clear','r') as file_ss:\n",
    "    ss_clear_lines = file_ss.readlines()\n",
    "\n",
    "pro_seq_ss_dict = {}\n",
    "for i in range(len(pro_clear_lines)):\n",
    "    pro = pro_clear_lines[i].strip('\\n').split('\\t')[0]\n",
    "    fold = pro_clear_lines[i].strip('\\n').split('\\t')[1]\n",
    "    seq = seq_clear_lines[i].strip('\\n')\n",
    "    ss = ss_clear_lines[i].strip('\\n')\n",
    "    pro_seq_ss_dict[pro] = [fold,seq,ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences amount in all: 8845\n",
      "Training sequences amount: 6191\n",
      "Validation sequences amount: 885\n",
      "Final training sequences amount: 7076\n",
      "Test sequences amount: 1769\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SeqSS_pro_list = list(pro_seq_ss_dict.keys())\n",
    "SeqSS_seq_num = len(SeqSS_pro_list)\n",
    "\n",
    "SeqSS_train_num_all = int(SeqSS_seq_num*0.8)\n",
    "SeqSS_train_num = int(SeqSS_seq_num*0.7)\n",
    "SeqSS_vali_num = SeqSS_train_num_all - SeqSS_train_num\n",
    "SeqSS_test_num = SeqSS_seq_num - SeqSS_train_num_all\n",
    "\n",
    "Resi_SS_dict = {'train_all':{'H':0,'E':0,'C':0},\n",
    "                'train':{'H':0,'E':0,'C':0},\n",
    "                'vali':{'H':0,'E':0,'C':0},\n",
    "                'test':{'H':0,'E':0,'C':0}}\n",
    "\n",
    "SeqSS_index_list = list(range(SeqSS_seq_num))\n",
    "shuffle_SeqSS_index_list = SeqSS_index_list[:]\n",
    "np.random.shuffle(shuffle_SeqSS_index_list)\n",
    "\n",
    "SeqSS_index_sample_all = shuffle_SeqSS_index_list[:SeqSS_train_num_all]\n",
    "SeqSS_index_sample_train = SeqSS_index_sample_all[:SeqSS_train_num]\n",
    "SeqSS_index_sample_vali = SeqSS_index_sample_all[SeqSS_train_num:]\n",
    "\n",
    "SeqSS_train_list_all = [SeqSS_pro_list[i] for i in SeqSS_index_sample_all]\n",
    "SeqSS_train_list = [SeqSS_pro_list[i] for i in SeqSS_index_sample_train]\n",
    "SeqSS_vali_list = [SeqSS_pro_list[i] for i in SeqSS_index_sample_vali]\n",
    "SeqSS_test_list = [SeqSS_pro_list[i] for i in SeqSS_index_list if not (i in SeqSS_index_sample_all)]\n",
    "\n",
    "SeqSS_Data_Path = '../Datasets/For_Seq_SS/'\n",
    " \n",
    "SeqSS_train_seq_all = open(SeqSS_Data_Path + 'SeqSS_seq_train_all','w')\n",
    "SeqSS_train_ss_all = open(SeqSS_Data_Path + 'SeqSS_ss_train_all','w')\n",
    "\n",
    "SeqSS_train_seq = open(SeqSS_Data_Path + 'SeqSS_seq_train','w')\n",
    "SeqSS_train_ss = open(SeqSS_Data_Path + 'SeqSS_ss_train','w')\n",
    "\n",
    "SeqSS_vali_seq = open(SeqSS_Data_Path + 'SeqSS_seq_vali','w')\n",
    "SeqSS_vali_ss = open(SeqSS_Data_Path + 'SeqSS_ss_vali','w')\n",
    "\n",
    "SeqSS_test_seq = open(SeqSS_Data_Path + 'SeqSS_seq_test','w')\n",
    "SeqSS_test_ss = open(SeqSS_Data_Path + 'SeqSS_ss_test','w')\n",
    "        \n",
    "for p in SeqSS_train_list_all:\n",
    "    SeqSS_train_seq_all.write(pro_seq_ss_dict[p][1] + '\\n')\n",
    "    SeqSS_train_ss_all.write(pro_seq_ss_dict[p][2] + '\\n')\n",
    "    \n",
    "    for ss_type in ['H','E','C']:\n",
    "        Resi_SS_dict['train_all'][ss_type] += pro_seq_ss_dict[p][2].count(ss_type)\n",
    "    \n",
    "for p in SeqSS_train_list:\n",
    "    SeqSS_train_seq.write(pro_seq_ss_dict[p][1] + '\\n')\n",
    "    SeqSS_train_ss.write(pro_seq_ss_dict[p][2] + '\\n')\n",
    "    \n",
    "    for ss_type in ['H','E','C']:\n",
    "        Resi_SS_dict['train'][ss_type] += pro_seq_ss_dict[p][2].count(ss_type)\n",
    "    \n",
    "for p in SeqSS_vali_list:\n",
    "    SeqSS_vali_seq.write(pro_seq_ss_dict[p][1] + '\\n')\n",
    "    SeqSS_vali_ss.write(pro_seq_ss_dict[p][2] + '\\n')\n",
    "    \n",
    "    for ss_type in ['H','E','C']:\n",
    "        Resi_SS_dict['vali'][ss_type] += pro_seq_ss_dict[p][2].count(ss_type)\n",
    "            \n",
    "for p in SeqSS_test_list:\n",
    "    SeqSS_test_seq.write(pro_seq_ss_dict[p][1] + '\\n')\n",
    "    SeqSS_test_ss.write(pro_seq_ss_dict[p][2] + '\\n')\n",
    "    \n",
    "    for ss_type in ['H','E','C']:\n",
    "        Resi_SS_dict['test'][ss_type] += pro_seq_ss_dict[p][2].count(ss_type)\n",
    "\n",
    "SeqSS_train_seq_all.close()\n",
    "SeqSS_train_ss_all.close()\n",
    "\n",
    "SeqSS_train_seq.close()\n",
    "SeqSS_train_ss.close()\n",
    "\n",
    "SeqSS_vali_seq.close()\n",
    "SeqSS_vali_ss.close()\n",
    "\n",
    "SeqSS_test_seq.close()\n",
    "SeqSS_test_ss.close()\n",
    "\n",
    "print('Sequences amount in all: %d'%SeqSS_seq_num)\n",
    "print('Training sequences amount: %d'%SeqSS_train_num)\n",
    "print('Validation sequences amount: %d'%SeqSS_vali_num)\n",
    "print('Final training sequences amount: %d'%SeqSS_train_num_all)\n",
    "print('Test sequences amount: %d'%SeqSS_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_all\n",
      "Residue amount: 1114086\n",
      "H: 410590 E: 251995 C: 451501\n",
      "\n",
      "train\n",
      "Residue amount: 975134\n",
      "H: 359465 E: 221108 C: 394561\n",
      "\n",
      "vali\n",
      "Residue amount: 138952\n",
      "H: 51125 E: 30887 C: 56940\n",
      "\n",
      "test\n",
      "Residue amount: 269936\n",
      "H: 97223 E: 61876 C: 110837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in Resi_SS_dict.keys():\n",
    "    print(k)\n",
    "    print('Residue amount: %d'%sum(list(Resi_SS_dict[k].values())))\n",
    "    print('%s: %d'%('H',Resi_SS_dict[k]['H']),'%s: %d'%('E',Resi_SS_dict[k]['E']),'%s: %d'%('C',Resi_SS_dict[k]['C']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SS_segment(seq,ss,rate=0.1):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for Seq-Fold models: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds amount in all: 420\n",
      "Sequences amount in all: 8132\n",
      "Training sequences amount: 6354\n",
      "Test sequences amount: 1778\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "    \n",
    "with open(Data_Path + 'fold_protein_dict_reduced.pickle', 'rb') as handle:\n",
    "    fold_protein_dict_reduced = pickle.load(handle)\n",
    "\n",
    "SF_seq_num = 0\n",
    "SF_train_num = 0\n",
    "SF_test_num = 0\n",
    "\n",
    "SF_train_dict = {}\n",
    "SF_test_dict = {}\n",
    "\n",
    "SF_Data_Path = '../Datasets/For_Seq_Stru/'\n",
    "\n",
    "SF_train_fold = open(SF_Data_Path + 'SF_fold_train','w') \n",
    "SF_train_seq = open(SF_Data_Path + 'SF_seq_train','w')\n",
    "SF_train_ss = open(SF_Data_Path + 'SF_ss_train','w')\n",
    "\n",
    "SF_test_fold = open(SF_Data_Path + 'SF_fold_test','w') \n",
    "SF_test_seq = open(SF_Data_Path + 'SF_seq_test','w')\n",
    "SF_test_ss = open(SF_Data_Path + 'SF_ss_test','w')\n",
    "        \n",
    "for f in fold_protein_dict_reduced.keys():\n",
    "    f_pro_list = [p for p in fold_protein_dict_reduced[f] if p in pro_seq_ss_dict.keys()]\n",
    "    f_pro_num = len(f_pro_list)\n",
    "    \n",
    "    if f_pro_num >= 3:\n",
    "        index_list = range(f_pro_num)\n",
    "        select_train_num = int(f_pro_num*0.8)\n",
    "    \n",
    "        SF_seq_num += f_pro_num\n",
    "        SF_train_num += select_train_num\n",
    "        SF_test_num += f_pro_num - select_train_num\n",
    "        \n",
    "        SF_index_list = list(range(f_pro_num))\n",
    "        shuffle_SF_index_list = SF_index_list[:]\n",
    "        np.random.shuffle(shuffle_SF_index_list)\n",
    "        \n",
    "        index_sample = shuffle_SF_index_list[:select_train_num]\n",
    "        train_pro = [f_pro_list[i] for i in index_sample]\n",
    "        test_pro = [f_pro_list[i] for i in index_list if not (i in index_sample)]\n",
    "        \n",
    "        SF_train_dict[f] = train_pro\n",
    "        SF_test_dict[f] = test_pro\n",
    "        \n",
    "        for p in train_pro:\n",
    "            SF_train_fold.write(pro_seq_ss_dict[p][0] + '\\n')\n",
    "            SF_train_seq.write(pro_seq_ss_dict[p][1] + '\\n')\n",
    "            SF_train_ss.write(pro_seq_ss_dict[p][2] + '\\n')\n",
    "            \n",
    "        for p in test_pro:\n",
    "            SF_test_fold.write(pro_seq_ss_dict[p][0] + '\\n')\n",
    "            SF_test_seq.write(pro_seq_ss_dict[p][1] + '\\n')\n",
    "            SF_test_ss.write(pro_seq_ss_dict[p][2] + '\\n')\n",
    "        \n",
    "        \n",
    "SF_train_fold.close()\n",
    "SF_train_seq.close()\n",
    "SF_train_ss.close()\n",
    "\n",
    "SF_test_fold.close() \n",
    "SF_test_seq.close()\n",
    "SF_test_ss.close()\n",
    "\n",
    "print('Folds amount in all: %d'%len(SF_train_dict.keys()))\n",
    "print('Sequences amount in all: %d'%SF_seq_num)\n",
    "print('Training sequences amount: %d'%SF_train_num)\n",
    "print('Test sequences amount: %d'%SF_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
